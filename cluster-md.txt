Abstract
--------

The cluster multi-device (Cluster MD) is a software based RAID
(only suitable for RAID1 now) storage solution for a cluster,
which provides the redundancy of RAID1 mirroring to the cluster.
When used with Corosync, Cluster MD supports distributed
high-availability Linux clusters. This chapter shows you how
to install and use Cluster MD.


Conceptual View
---------------

Cluster MD could protect RAID1 across cluster environment. If one
of the devices of the Cluster MD fails, it can be hot-replaced by
another device and it is re-synced to provide the same amount of
redundancy. The Cluster MD requires Distributed Lock Manager (DLM)
for co-ordination and messaging.

Figure:



Creating a clustered MD RAID device
-----------------------------------

Requirements

1. A running cluster with pacemaker
2. A resource agent fom DLM (Note, see the ocfs2 section on how to
		configure DLM).
3. At least two shared disk devices. You can use an additional device
as a spare which will failover automatically in case of device failure.


Before creating a clustered md device, make sure the DLM resource is up
and running. To create a device, issue the following command:

# mdadm --create md0 --bitmap=clustered --raid-devices=2 --level=mirror
 /dev/sda /dev/sdb

This will create a new clustered MD device and initiate a resync of the
two devices. A clustered bitmap is a device internal bitmap, with one
bitmaps for each node. You can monitor the progress of the resync in
/proc/mdstat. You can still use the device as the resync is being performed.

Other options

You can specify the following options while creating the clustered MD
device.

--nodes takes an argument for the maximum number of cluster nodes which can
be used with the device. If not specified, the default value is 4.

--home-cluster takes an argument for the cluster name. Not specifying
this option will cause mdadm to probe the cluster for the cluster name.

Note: Creating a clustered device disables incremental activation of the
device so that they are not automatically started by the inird system
and are restricted to start with resource agents only.

In order to create a cluster-md device with a spare for automatic
failover. Issue the following command:

# mdadm --create md0 --bitmap=clustered --raid-devices=2 --level=mirror
 --spare-devices=1 /dev/sda /dev/sdb /dev/sdc


Configuring a resource agent
----------------------------


Before creating a resource agent, edit /etc/mdadm.conf file to add the
device name and devices associated.

DEVICE /dev/sda /dev/sdb
ARRAY /dev/md0 UUID=1d70f103:49740ef1:af2afce5:fcf6a489

It may be advisable to add /etc/mdadm.conf in the csync tools list of
files so it is uniform in all nodes.

Configure a CRM resource as follows:

crm(live)configure# primitive raider Raid1 \
	params raidconf="/etc/mdadm.conf" raiddev=md0
	force_clones=true \
	op monitor timeout=20s interval=10 \
	op start timeout=20s interval=0 \
	op stop timeout=20s interval=0 


Ensure that the raider resource agent is activated cluster-wide by
adding it to the base-group which is cloned automatically.

crm(live)configure# group base-group dlm raider

Review your changes with show.

If everything seems correct, submit your changes with commit.


[Edit note: Change the device names as /dev/disk/by-id/.. for the
sake of uniqueness. It should be a SAN based name as opposed to a
virtual machine name.]


Adding a device
---------------

To add a device to an existing md-device, issue the command:

# mdadm --manage md127 --add /dev/sdc

Ensure that the device is "visible" on each node the md device is active,
or else the command will fail. Behaviour of the new device added depends
on the state of the MD cluster device. If only one of the mirrored
device is active, the new device becomes the second device of the
mirrored devices and a resync is initiated. If both devices of the
md-cluster are active, the new device added becomes a spare.

Architectural notes: This invokes a special message to all nodes to add
the new device. Each active node of the MD cluster device checks if the
device exists, and issues mdadm --manage /dev/md127 --cluster-confirm
<slot-number>:<device name>. This is performed automatically using a
udev script.

Re-adding a temporarily failed device
-------------------------------------

Quite often the failures are transient and limited to a single node. If
any of the node encounters a failure during an I/O operation, the device
will be marked as failed for the entire cluster. This could happen say, due
to a cable failure on one of the nodes. After correcting the problem,
you can re-add the device and only the portions of the device which are
out of sync will be synced (as opposed to syncing the entire device by
adding a new one). In order to re-add the device, issue:

# mdadm --manage /dev/md127 --re-add /dev/sdb


Removing a device
-----------------

Before hot-removing a device for replacement, make sure the device is
failed. This can be seen in /proc/mdstat with a (F) with the device. If
you wish to explicitly fail a device (say because the device is too old
or slow), you can perform it by issuing the following command:

# mdadm --manage /dev/md127 --fail /dev/sda

You can remove the failed device using the command:

# mdadm --manage /dev/md127 --remove /dev/sda



Note: cLVM chapter needs to be edited to include cluster md as one of
the options.
